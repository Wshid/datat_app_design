## [CHAP.01] 신뢰할 수 있고 확장가능하며 유지보수하기 쉬운 애플리케이션
### 데이터 중심적
- 많은 애플리케이션의 표준 구성 요소
  - 데이터베이스
    - 구동 앱이나, 타 앱에서 데이터를 재조회 가능하도록
  - 캐시
    - **읽기 속도 향상**을 위해, 비싼 수행 결과 기억
  - 검색 색인(search index)
    - **키워드**로 데이터 검색
    - 다양한 방법으로 **필터링**
  - 스트림 처리(stream processing)
    - **비동기** 처리를 위해 타 프로세스로 메세지 전송
  - 일괄 처리(batch processing)
    - 주기적으로 대량의 누적 데이터 분석

### 데이터 시스템에 대한 생각
- 데이터베이스, 큐, 캐시 등은 다른 범주가 아니다.
- 다양한 기술
  - 메세지 큐로 사용하는 `datastore` => `redis`
  - db처럼 지속성(durability)를 보장하는 큐 => `kafka`
  - 어플리케이션 관리 캐시 계층 => `memcached`
    - `elasticsearch`, `solr`
      - full-text 검색 서버

### 세가지 관심사
- 신뢰성(Reliability)
  - h/w, s/w 결함.
  - human error가 발생하더라도, 시스템은 **지속적으로 올바르게 동작**
    - 원하는 성능 수준에서 정확한 기능 수행
- 확장성(Scalability)
  - 데이터 양, 트래픽 양, **복잡도**가 증가하면서 이를 처리할 수 있는 적절한 방법
- 유지보수성(Maintainability)
  - 시간이 지남에 따라, 다양한 사람들이 작업 필요
  - 모든 사용자가 시스템 상에서 **생산적**으로 작업할 수 있어야 함

### 신뢰성
- 무언가 잘못되더라도 지속적으로 올바르게 동작함
- fault(결함)
  - 잘못될 수 있는 일
- fault-tolerant(내결함성), resilient(탄력성)
  - 결함을 예측하고 대처

### 결함 != 장애
- 결함
  - 사양에서 벗어난 시스템의 한 구성 요소
  - 발생 확률을 0으로 줄이는 것은 불가능
  - 결함으로 인해, 장애가 발생하지 않게 끔
    - **내결함성** 구조를 설계하는 것이 제일 좋음
  - 카오스 몽키(Chaos Monkey) - Netflix
    - 경고 없이 개별 프로세스를 무작위로 죽이는 것
      - 고의로 **결함**을 발생 시켜, 결함률을 증가시키는 방법
    - 고의적으로 결함을 유도하여, 내결함성 시스템을 지속적으로 훈련하고 테스트 하는 방법
- 장애
  - 사용자에게 필요한 서비스를 제공하지 못하고, 시스템이 멈춘 상태
- 보안문제
  - 예방책이 해결책보다 더 좋은 경우

### 하드웨어 결함
- 하드디스크 평균 장애 시간(mean time to failure, MTTF)
  - 약 10 ~ 50년
- 시스템 장애율을 줄이기 위한 대응
  - 하드웨어 구성요소의 **중복**(redundancy) 추가
- 디스크는 **RAID**구성
- 서버는 **이중 전원 디바이스**와 `hot-swap`이 가능한 CPU
- 하나가 죽을 경우, 구성 요소를 교체하는 동안 중복된 구성 요소를 사용
- 하드웨어 장애를 완벽히 막을 수 있는 방법은 아니나,
  - 이해하기 쉬우며, 장비가 중단되지 않고, 동작할 수 있게 함
- 하지만, 많은 수의 장비를 사용하게 되면서, 많은 장애 발생
  - aws의 경우, 가상 장비 인스턴스가 별도의 경고 없이 사용 불가한 상황 발생
  - 이런 플랫폼은 **단일 장비 신뢰성** < **유연성**(flexibility), **탄력성**(elasticity) 우선 처리
- 요즘 추세는
  - **소프트웨어 내결함성**기술을 사용하거나
  - **하드웨어 중복성**을 추가하여
    - 전체 장비의 손실을 견디는 시스템

### 소프트웨어 오류
- 하드웨어 결함은 **무작위적**이며 **독립적**
  - 한 장비의 디스크 결함시, 다른 장비의 디스크에 결함이 나지 않음
- **시스템 내 체계적 오류**(systematic error)
  - 예상하기 어려움
  - 하드웨어 결함보다, 시스템 오류를 더욱 많이 유발
    - 특정 입력이 있을 때, 모든 어플리케이션 서버 인스턴스가 죽는 소프트웨어 버그
    - cpu 시간, 메모리, 디스크 공간 등 **공유 자원**을 과하게 사용하는 프로세스
    - 시스템 속도가 느려저 반응이 없거나, 잘못 반환
  - 특정 상황이 발생하기 전까지, 나타나지 않음
- 신속한 해결책이 존재하지 않음

### 인적 오류
- 방지하는 방법
  - 오류의 기능성을 최소화한 설계
  - 사람이 실수하는 장소에서 장애 부분을 독립하기
    - `sandbox`등의 기능 제공
  - 인적 오류를 빠르고 쉽게 복구할수 있게 설계
  - 성능 지표와 오류율 같은 상세하고 명확한 모니터링 대책 마련

### 확장성
- 현재 안정적으로 동작한다고 해서, 미래에 안정적으로 동작한다고 보장할 수 없음
- **부하 증가**가 제일 흔한 이유
- 증가한 부하에 대한 시스템 능력을 설명하는 용어
- 하지만, 시스템에 부여하는 **일차원적인 표식**은 아님
- `X는 확장 가능하다`, `Y는 확장성이 없다`는 의미 없는 표현
- 확장성을 논하는 것
  - 시스템이 **특정 방식**으로 커지면 이에 대처하기 위한 선택
  - **추가 부하**를 다루기 위해 계산 자원을 어떻게 투입할지

### 부하 기술하기
- 시스템의 현재 부하 간결하게 기술
- 부하 성장 질문
  - 부하가 두배로 늘어난다면?
- **부하**는 **부하 매개변수**(load parameter)라 부르는 몇 가지로 나타낼 수 있다.
- 적합한 부하 매개변수의 선택, 시스템 설계에 따라 다름
- 부하 매개변수의 종류
  - 웹 서버의 초당 요청 수
  - 데이터베이스 read/write 비율
  - 동시 활성 사용자(active user)
  - 캐시 적중률
- **평균적인 경우** 또는 **소수의 극단적인 경우**가 병목현상의 원인
#### 트위터 예시
  - 주요 동작 두가지
    - 트윗 작성
      - 사용자가 follower에게 메세지 작성(avg 4.6k/sec, peak 12k/sec)
    - 홈 타임라인
      - 사용자는 팔로우한 사람이 작성한 트윗 보기 가능(300k/sec)
  - `12k/sec`의 write는 처리 가능하나,
  - 트위터의 확장성 문제는 트윗양이 아닌 `fan-out` 때문
    - **트랜잭션 처리 시스템**에서 **하나의 수신 요청**을 기다리는데 필요한  **다른 서비스의 요청 수**
  - 두가지 동작 구현 방식
    - 첫번째 방식
      ```sql
      SELECT tweets.*, users.* FROM tweets
        JOIN users ON tweets.sender_id = users.id
        JOIN follows ON follows.followee_id = users.id
        WHERE follows.follower_id = current_user
      ```
      - RDB의 처리방식
      - 트윗 작성
        - 새로운 트윗을, 트윗 전역 컬렉션에 삽입
      - 홈 타임라인 읽기 요청
        - 팔로우하는 모든 사람을 찾아, 해당 사람들의 모든 트윗을 시간순으로 정렬하여 합친다.
    - 두번째 방식
      - 각 수신자용 트윗 우편함처럼, 개별 사용자의 홈 타임라인 캐시 유지
      - 트윗 작성시
        - 해당 사용자를 팔로워하는 사람을 모두 찾아, 각자의 홈 타임라인 캐시에 새로운 트윗 삽입
      - 홈 타임라인 읽기 요청
        - 요청 결과가 미리 계산되어 있으므로, 비용은 저렴
  - `게시된 트윗의 평균 속도 = 홈 타임라인 읽기 속도 * 100`
  - 두번째 방식이 훨씬 효과적
  - **쓰기 시점**에 더 많은 일을 하고,
    - **읽기 시점**에 더 적은 일을 하는것이 맞음
  - 두번째 방식의 단점
    - 트윗 작성이 더 많은 **부가 작업**
  - 사용자당 팔로워의 분포
    - 팬 아웃 부하를 결정
    - **확장성**을 논의시, **핵심 부하 매개변수**
- 현재의 트위터 처리 방식(혼합형 접근 방식)
  - 대부분의 사용자 트윗은
    - 작성시 홈 타임라인에 펼쳐지지만,
  - 팔로워 수가 매우 많은 소수 사용자는 `fan-out`에서 제외
  - 사용자가 팔로우한 유명인의 트윗은
    - 별도로 가져와 **첫번째 방식**처럼 **읽는 시점**에 홈 타임라인에 합친다.

### 성능 기술하기
- **시스템 부하** 기술 시
  - 부하가 증가할 때, 어떤 일이 일어나는지 조사 가능
- 검토 방법
  - **부하 매개변수**를 증가시키고,
    - **시스템 자원**(cpu, mem, network bandwidth)는 변경하지 않고 유지하면 **시스템 성능**의 영향성 파악
  - **부하 매개변수**를 증가시켰을 때,
    - 성능이 변하지 않고 유지되길 바란다면, 얼마나 많이 **자원**을 늘려야할지
- 위 검토 방법의 두가지 모두 **성능 수치**가 필요함
- **hadoop**과 같은 일괄 처리 시스템은 **throughput**에 중점
  - 초당 처리할 수 있는 레코드 수 / 일정 크기의 데이터 집합으로 작업 수행시 걸리는 전체 시간
- **온라인 시스템**은 **response time**에 중점
  - C가 요청을 보내고 응답을 받는 사이의 시간
  - **응답 시간**은 단일 숫자가 아닌, **분포** 값으로 측정
    - C의 요청별로 달라지기 때문
- **지연 시간** vs **응답 시간**
  - 응답 시간
    - C의 관점에서 본 시간
    - 요청을 처리하는 **실제 시간**(서비스 시간) + **네트워크 지연** / **큐 지연**
  - 지연 시간
    - 요청이 처리되길 기다리는 시간
    - 서비스를 기다리며 latent(휴지)상태인 시간
- 특정 요청이 오래 걸리는 경우
  - `outlier`라고 함
  - 원인 추정
    - bg process의 **context switch**
    - 네트워크 패킷 손실과 TCP 재전송
    - GC pause
    - page fault // 디스크 강제 읽기 발생
    - 서버 랙의 기계적인 진동 등
- 평균과 중앙 값
  - 평균은 좋은 지표가 아님
    - 산술 평균(arithmetic mean, `sum(n) / n`)
    - 실제로 얼마나 많은 사용자가 지연을 경험했는지 알 수 없음
  - 중앙값(median)
    - 실제 요청들의 중간 지점
    - 사용자가 얼마나 오래 기다렸는지에 대한 확인 가능
    - 백분위 사용, `p50`지표
    - 단일 요청을 참고함
      - 세션 내 여러 요청을 종속하지 않음

### 꼬리 지연시간(tail latency)
- 상위 백분위 응답 시간
- 사용자 경험에 직접적
- aws의 경우, 응답 시간 요구사항을 **99.9분위**로 기술
  - 1000개 중, 1개만 영향이 있음을 의미
- 보통 느린 요청을 경험한 고객은, 그 만큼 계정에 많은 데이터를 가진 **hard user**
- **99.99분위**
  - `1/10,000` 영향
  - 너무 많은 비용 발생

### 백분위와 SLO/SLA
- 백분위를 많이 사용하는 곳
  - SLO(Service Level Objective, 서비스 수준 목표)
  - SLA(Service Level Agreement, 서비스 수준 협약서)
  - 기대 성능과 서비스 가용성을 정의하는 계약서
- SLA 예시
  - 응답 시간 median < 200ms, p99 < 1sec, 정상 서비스 상태 간주
  - 서비스 제공시간은 99.9% 이상이어야 함

### 큐 대기 지연
- queueing delay
- 높은 백분위에서 **응답 시간의 상당 수**를 차지함
- **head-of-line blocking**(선두 차단)
  - 서버는 병렬로 **소수의 작업**을 처리 가능하기 때문에(CPU core 제한 등)
    - 소수에 느린 요청 처리만으로도, 후속 처리가 지체
  - 후속 요청이 빨리 처리되더라도, 이전 요청 처리 대기 시간 때문에
    - C는 전체적으로 응답시간이 **느리다**라고 판단
- 따라서 C의 응답 시간 측정은 중요

### 시스템 확장성 테스트시 유의점
- 확장성 테스트를 위한, 인위적 부하 생성시,
  - 부하 생성 클라이언트는
  - **응답 시간과 독립적**으로 요청을 지속적으로 보내야 함
- C가 다음 요청을 보내기 전에, 이전 요청이 완료되길 기다리면
  - **테스트에서 인위적으로 대기시간**을 실제보다 더 짧게 만들어 평가 왜곡

### 실전 백분위
- **단일 최종 사용자 요청**의 일부로써
  - 여러번 호출되는 **백엔드 서비스**에서 특히 중요
- 꼬리 지연 증폭(tail latency amplification)
  - **병렬**로 호출해도
    - 최종 사용자 요청은 여전히 병렬 호출중 **가장 느린 호출이 완료**되길 기다려야 함
  - **하나의 요청으로도, 전체 사용자 요청이 느려질 수 있다.**
- 서비스의 모니터링 대시보드에
  - **응답 시간 백분위**를 추가하려면,
  - 지속적으로 백분위를 효율적으로 계싼해야 함
- 예시
  - 지난 10분간 요청의 **응답 시간**을 `rolling window`로 유지하려면
  - 1분마다 구간내 `median`과 다양한 백분위를 계산하여 각 지표를 그래프에 그린다.
- 효율적인 방법
  - 시간 구간 내 모든 요청의 응답 시간 목록을 유지하고
  - 1분마다 목록을 정렬하는 방법
  - 너무 비효율적일 경우
  - 상황에 따라 `forward decay`, `t-digest`, `HdrHistorigram` 사용
    - `cpu`와 메모리 비용을 최소로 하면서 좋은 백분위 근사치 계산 가능
- **백분위 평균(시간 해상도를 줄이는 경우, 여러 장비의 데이터 결합 등) 수학적으로 의미가 없음**
  - 응답 시간 데이터를 집계하는 올바른 방법은 **히스토그램**을 추가하는 것

### 부하 대응 접근 방식
- **부하 매개변수**가 어느 정도 증가하더라도 **좋은 성능**을 유지하려면?
- 확장성
  - **scaling up**
    - 용량 확장, 수직 확장(vertical scaling)
    - 성능이 좋은 장비로 업그레이드
  - **scaling out**
    - 규모 확장, 수평 확장(horzontal scaling)
    - 다수의 낮은 사양 장비 부하 분산
      - 이를 `shared-nothing architecture`(비공유 아키텍처)
- **단일 장비**에서 수행될 수 있는 시스템은
  - 간단하지만
  - 고사양 장비는 비싸기 때문에
    - 집약된 작업 부하는, 규모 확장을 피하지 못함
  - `적절한 사양의 장비 몇 대 > 낮은 사향 가상 장비 여러대`
    - 간단하고 저렴
- `elastic`한 시스템
  - 탄력적
  - 부하 증가를 감지하면 **컴퓨팅 자원**을 자동ㅇ으로 추가
  - 부하를 예측할 수 없을만큼 **높은 경우 유용**
  - 하지만 **수동으로 확장하는 시스템이 더 간단**하고, 운영상 예상치 못한일이 더 적음

### stateless와 scaling up
- 다수의 장비에 배포하기 쉬움
-  단일 노드에 `stateful` 데이터 시스템을 **분산 설치**하는 일은
   -  많은 복잡도 발생
- 확장 비용이나 데이터베이스를 **분산 운용**해야 하는 **고가용성 요구**가 있을 때까지
  - **단일 노드**에 데이터베이스를 유지하는 것(scaling up)이 통념

### one-size-fits-all 확장 아키텍처의 존재
- 범용적이고 모든 상황에 맞는 확장 아키텍처는 없음
- 아키텍처 결정 요소
  - 읽기의 양
  - 쓰기의 양
  - 저장할 데이터의 양
  - 데이터 복잡도
  - 응답 시간 요구 사항
  - 접근 패턴
  - ...

### 유지보수성
- 소프트웨어 **비용의 대부분**
  - 초기 개발에서 비용이 많이 투자되지 않음
- 유지보수의 종류
  - 버그 수정, 시스템 운영 유지, 장애 조사, 새로운 플랫폼 적용, ...

### 소프트웨어 설계 원칙(3)
- 운용성(operability)
  - 시스템을 원활하게 **운영할 수 있도록 쉽게**
- 단순성(simplicity)
  - 복잡도를 최대한 제거
  - 시스템을 **이해하기 쉽게**
- 발전성(evolvability)
  - 이후에 시스템을 **쉽게 변경할 수 있게**
  - 다음과 같은 의미
    - **유연성(extensibility)**, **수정 가능성(modifiability)**, **적응성(plasticity)**

### 운용성 : 운영의 편리함 만들기
- 나쁘거나 불완전한 **소프트웨어 제약** 회피
- **좋은 소프트웨어라도 나쁘게 운영할 경우, 작동을 신뢰할 수 없음**
- 운영중 일부 측면을 **자동화** 하여야 하나,
  - 자동화의 첫단계는 사람

### 단순성 : 복잡도 관리
- 프로젝트가 커짐에 따라 시스템은 복잡하고 이해하기 어려움
- 복잡도가 높은 소프트웨어 프로젝트를
  - `big ball of mud`(커다란 진흙 덩어리)로 묘사
- 복잡도의 증상
  - 상태 공간의 급증
  - 모듈간 강한 커플링(tight coupling)
  - 복잡한 의존성
  - 일관성 없는 명명(naming)과 용어
  - 성능 문제 해결을 목표로 한 해킹
  - 임시방편의 문제를 해결한 특수 사례(spacial-casing)
- **단순성**이 구축하려는 시스템의 목표여야 함
  - 단, `단순 != 기능 줄이기`
- **추상화**
  - 우발적 복잡도를 제거하기 위함
  - 많은 세부 구현을 줄일 수 있음

### 발전성 : 변화를 쉽게 만들기
- agile 작업 패턴
  - 변화에 적응하기 위한 프레임 워크
  - **테스트 주도 개발(test-driven development, TDD**)
  - **리팩토링(refactoring)**
- 데이터 시스템 변경을 **쉽게**하고, 요구사항에 시스템을 맞추는 방법은
  - 시스템의 **간단화**와 **추상화**와 밀접한 관련이 있음

### 정리
- 어플리케이션이 유용하려면 여러 **요구사항**을 만족 시켜야 함
- 요구사항의 종류
  - 기능적 요구사항
    - 여러 방법으로 데이터를 **저장하고**, **조회하고**, **검색하고**, **처리**하게끔 허용하는 작업과 같이 해야 하는 일
  - 비기능적 요구사항
    - 보안, 신뢰성, 법규 준수, 확장성, 호환성, 유지보수성과 같은 일반 속성
- 신뢰성
  - **결함**이 발생해도, 시스템이 올바르게 동작
- 결함
  - 무작위적이고, 비상관관계의 **하드웨어**
  - 체계적이고 다루기 어려운 **소프트웨어 버그**
  - 불가피하게 실수를 하는 **사람**
- **내결함성** 기술은
  - 최종 사용자에게 특정 유형의 **결함을 숨김**
- 확장성
  - **부하가 증가**해도 **좋은 성능**을 유지하기 위한 전략
  - **양적으로** 부하와 성능을 설명하는 방법이 선행되어야 함
    - 성능 측정 방법 : 응답시간 백분위
  - 확장 가능한 시스템에서는
    - **부하**가 높은 상태에서
    - **신뢰성**을 유지하기 위해, 처리 용량을 증가시킬 수 있음
- 유지보수성
  - 좋은 **추상화**는 **복잡도**를 줄이고, 쉽게 **시스템을 변경할 수 있음**
  - 좋은 **운용성**이란
    - 시스템의 건강 상태 잘 관찰
    - 시스템을 효율적으로 관리하는 방법 보유